













/*
trail notes
you've been doing a great job of making each of these a one screen show!

[x]check
[x]integer, text convert
[x]data, text convert
[x]string clip to parse
[x]string validation, like only alphanumeric, regex from chatgpt
[x]randbetween, inclusive, just in case you need it
[x]ticks to nice text for the user in the current time zone, see if you can avoid a library actually--yes, you can avoid a library and do all the internationalization you need, and very succinctly
[x]say four digit byte size, don't otherwise internationalize numbers

[x]browser crypto, symmetric and asymetric, []hashing
[~]uri encoding, get taht in here just for compleness, have tests that show one+space or one%20space
[]email validation, another one for chatgpt, 1raw, 2cleaned for use, 3normalized for duplicate detection

do all the text checking you need for the database
and for validating single-line user input, like a screen name
not, at this time, the text checking for a user writing a comment


*/



/*
you want to use tag and now as local variables in code
but they're clashing with tag() and now() as short commonly imported functions

you were thinking makeTag(), but tickNow() is not awesome
maybe Tag() and Now()



*/





once you've figured out pinia, you might code the sorted record thing
allows immediate lookups by tag
and also browsing in tick count order
keeps sorted
fast for adding already sorted input data
doesn't readmit duplicates, which will be super common
or maybe pinia already has something better
or maybe this isn't the right way to do things



/*
TODO be able to manually run all the tests these places:
[x]icarus, vite
[]nuxt, local, client-side
[]nuxt, local, server-side, emulated worker
[]nuxt, deployed, client-side
[]nuxt, deployed, server-side, deployed worker
[]lambda, local, server-side, emulated lambda
[]lambda, deployed, server-side, deployed lambda
*/























//maybe wrap json parse and stringify to deal with errors you've encountered





//    _                 
//   (_)___  ___  _ __  
//   | / __|/ _ \| '_ \ 
//   | \__ \ (_) | | | |
//  _/ |___/\___/|_| |_|
// |__/                 

export function jsonStringify(o) {//watch out for a circular reference
	/*
//watch out for a circular reference
try {
return JSON.stringify(o, null)//single line
} catch (e) { return '(circular reference)' }//watch out for circular references
*/
}

export function jsonParse(s) {//watch out for a blank body
	/*
//watch out for a blank body
//get the text first, and keep that, too

r.responseText = await response.text()//might be nothing, even on success
if (r.responseText) r.responseData = JSON.parse(r.responseText)//throws if you give it nothing
*/

}



























/*
2024apr26
code encoding, some bikeshedding

[x]from separate project entry point, Ctrl+S leads to green or red check on page

library0 ~

[]base16 data
[]base64 data
[]base62 data, your code
[]base62 integer, chatGPT's code

[]round trip check
[]toss on failure
[]speed test your base62 with the browser's base64

library1 ~

[]random 1-9
[]random 0-9
[]random 0-9 A-Z a-z

not in scope for this first pass:
-page rendered width analysis
-unicode search for narrow accents that still render
-unified chinese base256





*/























/*
2024may16
dustiest corner of the bike shed
make sure your log and see plan is good before you go back in there
-get rid of log destinations
-do program an indented deep object sayer
-dont use it when there's a browser inspector which has arrows




*/




/*
more bikeshedding here, but what if log worked like this
log(a)
turns a into text, and prefixes it with the timestamp
log(a, b)
not sure anymore
the thing you forgot when designing this refactor above was that in the browser, you don't want everything text, because the browser inspector has arrows to go deep into nested objects, which you won't code yourself, and which is incredible




*/


/*
moar notes and ideas

instead of see doing lines like
(string) s "hello"
all you need is "hello" because quotes mean string
{object}
[array]
"string"
7 with no punctuation is a number
true with no punctuation is a boolean


you made the separte vite entry point
but now nuxt localhost:3000/test is working so well you don't need it(?)
like, it's just as fast
and you can Ctrl+S library0.js several times in a row and it refreshes each time
and, the text stays in place on the page, and also scrolls down in the console
so, what more is there to do here?
i guess refactor it so you don't have log destinations anymore

*/






/*
TODO
see is too confusing; your planned replacement will be called inspect
so then the core functions are log, say, and inspect
*/
/*
export function see(...a) {//see into things, including key name, type, and value
	let s = ''
	for (let i = 0; i < a.length; i++) {
		s += (a.length > 1 ? newline : '') + _see2(a[i])//put multiple arguments on separate lines
	}
	return s
}
function _see2(o) {
	let s = ''
	if (o instanceof Error) {
		s = '(error) ' + o.stack//errors have their information here
	} else if (Array.isArray(o)) {
		s = `(array) [${o}]`
	} else if (typeof o == 'object') {
		s += '(object) {'
		let first = true
		for (let k in o) {
			if (!first) { s += ', ' } else { first = false }//separate with commas, but not first
			s += `${k} (${typeof o[k]}) ${_see3(o[k])}`
		}
		s += '}'
	} else {
		s = `(${typeof o}) ${_see3(o)}`
	}
	return s
}
function _see3(o) {
	try {
		return JSON.stringify(o, null)//single line
	} catch (e) { return '(circular reference)' }//watch out for circular references
}
*/




/*
todo, javascript is so wonky it's hard to make a see that lets you easily look inside
here are some possible improvements
get things on multiple lines and indent them 2 spaces
there's lots of:

ok (boolean) true
n (number) 7
responseText (string) ""
clone (function) undefined
o (object) {message (string) "hello"...}
a (array) [1,2,3]

shorten this so it's just:

ok: true,
n: 7,
responseText: "",
clone(),
o {
	message: "hello"
	...
},
a [1,2,3]

because json stringify skips functions, you'll probably have to write your own recursive walker and indenter
and that's fine, go three levels deep by default

moar notes for your return to this bike shed:

remove look(o), instead you have to call log(see(o))

remove the multiple loggers feature
you want icarus, but will do that later as a separate system
that system will have auto refresh
in-place traffic lights
and an in-place log that is maybe icarus()

you just remembered that log shouldn't turn everything into a string
doing so breaks the inspector's > arrow
this may also be why earlier log didn't tickstamp
if log gets a single string, do it all on one line
if log gets a single non-string, or multiple anything, call console.log multiple times for multiple lines
*/








do encoding at the same time as you do cryptography
maybe make a little object so you dont have to type Uint8array over and over again
the object is indelible
holds some binary data
and has methods to convert between all these different formats:

text, like a string of cleartext
length, how long is it in bytes when you look at it as bytes
base16
base64, checking roundtrips and whatnot

you create these from:
text strings
base16
base64
random data, cryptographically secure

following hte model, this should be tiny, it shouldn't be your node project's Data object



remind yourself of the pattern, but you think you remember it
and now you can use const!

export function Data(c) {
	const o = {}
	o.type = 'Data'//no how do you make that const?
	const type 
	

	const function fromText(s) {

	}

	const function base16()
	const function text()

	return o
}
this constructor makes a new data object

have it lazily create and then save the different forms
understand how utf8 text encoding plays a role with all this, bake that into Data as the default









server logs
dog() and log() goes to datadog
or, figure out your pattern for an api function
with a total covereage try catch
and then maybe if there's an error, inspect it into text and send it back to the browser
and even use status codes, maybe--ask chatgpt
first just see if you can get a stack trace from a worker
but then you have to make sure this isn't an attack vector, you have to know if you're built for production or not

the api full catch also dogs if it takes longer than 2s

a little clock appears in the page at 2s indicating (clock) working 2s...

dog is great because it's secure and private even in deployment--if the attacker figures out how to break an api, they can't get information mistakingly included in a log

duh the workign clock can totally be completely client-side
ask chatgpt if there's a convention in nuxt to guard against repeat requests to the same api
the don't click or you'll mess up your order, thing

ask chatgpt about the confirm form resubmission browser warning
you don't think you're coding any forms that could cause this, but understand it for the first time ever to be sure









/*
this one is for birthdates, stored in the database as text like "1980-02-14"
take the user's input free-form
allow months as feb, 2, 02, whatever
allow years 02, 2002, 76, whatever
figure out what they may mean and put up one or more buttons that snap to

you also need to take a well formatted one, and return the year, month, day as numbers

Date of birth
You must be 18+
Enter like YYYYMMDD four digit year, two digit month, two digit day
Tell us your real birthday!
Not posted on profile, kept private and secure
We check IDs for creators, date must match
Got that as January 22, 1999

If you become a creator
Must match your birthdate on your ID
Private, secure, not posted on profile
We won't post on profile
Must match your ID
Kept private from fans and collaborators
It'll
Birthday with 
that's February 14, 1980


have icarus have a little expandable text box where you can live type into a function to test it



idea for how to get dates the fastest
worst, by the way, is putting up a calendar clicker for today, asking for a dob
phone puts up number pad
instruction text says birthday, YYYYMMDD
see if you can infer other orders, including shorter orders like 040176
if they type YYMMDD, YYYYDDMM
one or two buttons pop up that say "do you mean 2002 Jan 26"
if it's ambiguous, multiple buttons pop up
a single backspace clears the whole field, maybe
*/







/*
//here's a bike shed--write a tight little deindent


so you can
{
	{
		s = `
		first list
		second line
		third line
		`
	}
}

and what you get is
`first line
second line
third line
`

so it removes the leading newline
keeps the other newlines the same
and removes whatever space is at the start of the first line
from all the other lines

maybe remove that number of things
like that number of

confirm you don't chop off any nonwhitespace doing this


*/






/*
function f() {

	const alphabet = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz';
	let census = {}
	for (let c of alphabet) census[c] = 0

	let duration = 1*Time.second
	let t = now()
	let d, b
	while (now() < t + duration) {
		d = Data({random: randomBetween(1, 32)})
		b = d.base62()
		for (let c of b) census[c]++
	}

	let s = ''
	for (let c of alphabet) s += `\r\n${c}\t${census[c]}`
	log(s)


}
//f()

test(() => {


})
*/







/*
noop(() => {

	//get some random bytes
	let b16 = Data({random: 40}).base16()
	//encode into base62
	let b62 = arrayToBase62(Data({base16: b16}).array())
	//and back again
	let b16a = Data({array: base62ToArray(b62)}).base16()
	//log to confirm looks good
	log(b62, b16, b16a)


})


noop(() => {

	let d = Data({random: 40})
	let b62 = _arrayToBase62(d.array())
	log(
		b62,
		d.base16(),
		Data({array: _base62ToArray1(b62)}).base16(),
		Data({array: _base62ToArray2(b62)}).base16()
	)


})


/*
async function f() {
	let d, s
	let size = 50
	let duration = 4*Time.second


	let r1 = 0
	let t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = arrayToBase64(d.array())
		base64ToArray(s)
		r1++
	}
	log(r1+' base64')

	let r2 = 0
	t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = _arrayToBase62(d.array())
		_base62ToArray1(s)
		r2++
	}
	log(r2+' base62 to array method 1')

	let r3 = 0
	t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = _arrayToBase62(d.array())
		_base62ToArray2(s)
		r3++
	}
	log(r3+' base62 to array method 2')
}
f()
*/








/*
async function f2() {
	let k = await symmetricCreateKey()
	let b = await symmetricExportKey(k)
	let k2 = await symmetricImportKey(b)
	let p = 'a short message, like card info'
	let c = await encrypt(p, k)
	let d = await decrypt(c, k)
	if (d != p) log('decryption mismatch')
}
async function f3() {
	let k = await createKey_new()
	let b = await exportKey_new(k)
	let k2 = await importKey_new(b)
	let p = 'a short message, like card info'
	let c = await encrypt_new(p, k)
	let d = await decrypt_new(c, k)
	if (d.text() != p) log('decryption mismatch')
}
async function f() {
	log('just in a function f')
	let r1 = 0
	let t = now()
	while (now() < t + 4*Time.second) {
		r1++
	}
	log(r1+' empty')
	let r2 = 0
	t = now()
	while (now() < t + 4*Time.second) {
		await f2()
		r2++
	}
	log(r2+' direct')
	let r3 = 0
	t = now()
	while (now() < t + 4*Time.second) {
		await f3()
		r3++
	}
	log(r3+' custom')
}
f()
*/



/*
improve tiny tests

TODO make this an object that fills up here
but which is exported, and you can bring into a vue component
and run tests there and see results

also, get tests to await async tests etc correctly
this should be pretty easy

also, did you have mustThrow or something before? you need that again; right now you're just skipping writing tests of check* functions

and have it return an object of stats and outcomes, one of which is the composed status line
*/





/*
for inspect()
TODO never add these additional features, because this bike shed is fancy enough:
-recurse, indenting two spaces, stopping if the text grows above 2kb
-deal with tabs and newlines in function definitions and the error stack trace
-show short arrays and objects on a single line; long ones on multiple indented lines
-say the length of a very long array, showing starting and ending items with an ellipsis in the middle

short notes relevant to those never-do improvements
stack trace lines start with four spaces, maybe just remove them
spaces and tabs in function code come through, maybe trim them and separate with the pilcrow
*/
test(() => {
	//early inspect practice
	/*
	let b = true
	let s = 'hello'
	let n = 721
	let a = ["p", "q", "r"]
	let o = {
		name: 'apple',
		quantity: 11,
		f: (()=>{}),
		below: {
			block: 'bedrock'
		}
	}
	let e = (() => {
		let o = {}
		try {
			o.notThere.andBeyond
		} catch (e) {
			return e
		}
	})()
	log(inspect(b, s, n, a, o, e))
	log(inspect(b, s, {n, a, o, e}))
	*/
})
test(() => {
	//more recent inspect practice
	/*
	log(inspect(true))//boolean
	log(inspect(7))//number
	log(inspect('hello'))//string
	log(inspect(['a', 'b', 'c']))//array
	log(inspect({key1: 'value1', key2: 'value2'}))//object

	let f1 = function namedFunction() {
		let i = 7
		return 'named function result'
	}
	let f2 = function() { return 'anonymous function result' }
	log(inspect(f1))//function
	log(inspect(f2))//function

	try {
		notDefined
	} catch (e) {
		log(inspect(e))//error with stack trace
	}
	*/
})




TODO do you need soft versions of the text functions?

//TODO these throw if anything is out of bounds, maybe add startSoft, endSoft, beyondSoft that instead return shorter or blank
//bookmark you added this really fast, go through the larger process of actually testing them, and them using them where you need them
export function startSoft(s, n)  { return clipSoft(s, 0, n) }
export function clipSoft(s, i, n) {
	s = s+''//fix everything
	if (!minInt(i)) i = 0
	if (!minInt(n)) n = 0

	if (i     > s.length) i = s.length
	if (i + n > s.length) n = s.length - i

	return clip(s, i, n)
}
test(function() {
	ok(clipSoft('abc', 1, 0) == '')
	ok(clipSoft('abc', 1, 1) == 'b')
	ok(clipSoft('abc', 1, 2) == 'bc')
	ok(clipSoft('abc', 1, 3) == 'bc')
});


/*
TODO
consider this pattern

checkThing - throws if not
isThing - false if not
makeThing - shapes to bring into compliance and returns

then you put all the tests around isThing
and instead of *Soft here, you use isThing when you don't want an exception
*/


//make Process and Fetch that work node or web worker, vue front or back end
const Access = (typeof process != 'undefined' && process.env) ? process.env : import.meta.env
const Fetch = (typeof fetch != 'undefined') ? fetch : (await import('node-fetch')).default









== browser fingerprint

the browser key in local stroage can keep the user signed in forever
but, an attacker could coach a n00b through discord to share it with him
so also use a browser fingerprint
you're looking for something script can't affect, and which doesn't change
even as the device moves geographically and goes through browser and os updates

WebGL Renderer: NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2
WebGL Vendor: NVIDIA Corporation
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
Screen Resolution: 1920x1080

don't do screen resolution because multiple monitors and turning your phone
add window.location to get https://cold3.cc, make sure there's not a page afterwards
and add the browser tag
so you're hashing text like this:

agent:Mozilla/. (Macintosh; Intel Mac OS X __) AppleWebKit/. (KHTML, like Gecko) Chrome/... Safari/.;renderer:NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2;vendor:NVIDIA Corporation;tag:lYPvabMNWoWwlrgkguy0g;

you make and save tag if it's not already there before doing this
you never send tag over the wire, just the hash
you never save that hash to disk, just the wire
yeah, this is a great idea
keep the user signed in forever, (unless he upgrades his video card)
but also make it so a reddit coach can't get a n00b victim to look in local storage and share his browser tag

maybe even call it
account-access-tag-DO-NOT-SHARE

current_session_password: account_access_code_DO_NOT_SHARE_lYPvabMNWoWwlrgkguy0g

what if you did that, except for user agent, you remove all the numerals

also, save it in a global variable so you only have to hash it once on page load, not every time you make a request

async function getBrowserFingerprint() {
  // Get WebGL Renderer and Vendor
  const getWebGLFingerprint = () => {
    const canvas = document.createElement('canvas');
    const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
    if (!gl) return null;

    const debugInfo = gl.getExtension('WEBGL_debug_renderer_info');
    if (!debugInfo) return null;

    const renderer = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL);
    const vendor = gl.getParameter(debugInfo.UNMASKED_VENDOR_WEBGL);

    return {
      renderer: renderer,
      vendor: vendor
    };
  };

  // Get User Agent
  const userAgent = navigator.userAgent;

  // Get Screen Resolution
  const screenResolution = `${screen.width}x${screen.height}`;

  // Combine all pieces of information
  const webGLFingerprint = getWebGLFingerprint();
  if (!webGLFingerprint) return "WebGL information not available";

  const combinedFingerprint = `
    WebGL Renderer: ${webGLFingerprint.renderer}
    WebGL Vendor: ${webGLFingerprint.vendor}
    User Agent: ${userAgent}
    Screen Resolution: ${screenResolution}
  `;

  return combinedFingerprint.trim();
}

// Example usage
getBrowserFingerprint().then(fingerprint => {
  console.log(fingerprint);
});











== moar web security, CORS

net23 apis will check the origin header right in the lambda node code
you should also make cold3.cc/api apis only talk to cold3.cc and net23 on https
you could check origin in code, but a more standard way is to implement the CORS preflight thing
and a standard way to do that is:
https://nuxt-security.vercel.app/documentation/middleware/cors-handler
"Testing CORS configuration can be done by running one application with NuxtSecurity enabled and creating a second application (that is running on a different port) that will send a request to the first app. Then, a CORS error could be easily observed that proves that CORS is working as expected."

if you've got things all CORS-ed up, can you still develop on localhost?






== moar web security, CSP

chatgpt suggests:

<head>
  <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self'; style-src 'self'; object-src 'none'; frame-ancestors 'none'">
</head>

but not sure what this prevents. not injecting script directly into the page through inspector, but maybe a chrome extension running script?
also not sure what adding this could break








== iphone faceid for the web

for four years now you've been able to do faceid on the web!
but you've never seen it anywhere!
and, it's easy!

https://developer.apple.com/videos/play/wwdc2020/10670/
https://webkit.org/blog/11312/meet-face-id-and-touch-id-for-the-web/
https://support.apple.com/en-us/HT213449

const options = {
    publicKey: {
        rp: { name: "example.com" },
        user: {
            name: "john.appleseed@example.com",
            id: userIdBuffer,
            displayName: "John Appleseed"
        },
        pubKeyCredParams: [ { type: "public-key", alg: -7 } ],
        challenge: challengeBuffer,
        authenticatorSelection: { authenticatorAttachment: "platform" }
    }
};
const publicKeyCredential = await navigator.credentials.create(options);

Registration (Face ID Setup)
-The user clicks the "Register Face ID" button.
-A new credential is created using navigator.credentials.create().
-The options include information about the relying party (your website), the user, and the desired credential parameters.
-The public key credential is logged and should be sent to your server for storage.

Authentication (Using Face ID)
-The user clicks the "Authenticate with Face ID" button.
-The user's Face ID is authenticated using navigator.credentials.get().
-The options include a challenge and the allowed credentials (user's registered credential ID).
-The assertion is logged and should be sent to your server for verification.






== four similar but distinct tasks

1 logs for development (logflare)
only use while coding
console.log works great locally, but not pushed
confirm that you're getting the same error both places
doesn't need to be permanent
needs to be absolutely reliable and instantaneous

2 logs for production (datadog)
use in production
keep a permanent record of all the details that happened with every third party api
do more analysis later, asking questions then you haven't though of now

3 uptime checking (pingdom, datadog synthetics, checkly)
know if cloudflare pages, workers, lambda, or supabase goes down or becomes slow
be on pagerduty to confirm and complain
if there's an outage in the middle of the night, know how long it lasted, and how frequently this happens

4 round robin control, api performance, and fault alerting (kinesis, datadog, kafka)
balance load between two equivalent third party apis, like for sending SMS, or charging credit cards
immediately stop using a service that starts rejecting use, or no users can complete tasks on
use for third party services that are easy to stack redundantly and likely to break or be unreliable
see a dashboard of median response time and processing time of api calls and database queries

and you started into kinesis by saying imagine you're sending it a stream of numbers, and want the hourly average

https://docs.aws.amazon.com/msk/latest/developerguide/serverless.html
is fast, serverless, and lets you query by timestamp range (kinesis just lets you look at the last 5mb or whatever)

>next morning thinking

[0] logs for development
use logflare, it's simple and faster than datadog

[1] downtime and pagerduty for pages, workers, and supabase
a provider like pingdom is setup for sanity checking, is my site online
for having their own dashboard put up a graph for, how fast is page load? hello world api? increment count api? testing cloudflare pages, workers, and supabase speeds
sends you a pagerduty alert (can they do telegram?) if cloudflare pages, workers, or supabase are down
but this doesn't test speeds as encountered by real users, and that's fine, you trust the providers to be good about that

[2] redundant fragile apis like sms, email, credit cards. characteristics:
these have the following properties:
likely to be slow and unreliable
likely to stop working for a day, or forever
easy to place redundant ones in a list and use round-robin
some also have user interaction steps, you only know a sms got through when the user types it in (and can only compare speeds when you've got really a lot of users doing this)

[2b] needs
here, the system needs a few things:
(i) real time knowledge when one breaks, not to tell a human, rather to automatically steer around
(ii) auditable record of everything we told the api, and everything it said back, to use later maybe

[2c] datadog for audit logs
datadog is for historic logs
every api interaction, you dog() everything you told the api, and everything it said back
you don't use this, but it's there for later, if you need it

[2d] sql pattern for round robin visibility
you make a pattern in supabase to track the minute to minute speed and reliability of fragile apis
a table is really simple, just a tick count, a millisecond duration, and success or failure
and then that table is named, like the name of one provider in one category, like aws email, or braintree cards
these tables only hold the last hour or two of data
if something is very rare, it also holds the last 1000 records or whatever
you call it with a function like robin(name, duration, outcome) and it adds a row, finding the right table
you query it with another function, and it tells you the average, median, fastest and slowest N events, and if there are a bunch of recent failures, like it's been broken for 20 minutes and counting, or something
when a call comes in and sees that it's the first call in a short time period (one minute? 6 minutes?) it automatically performs the totalling step
this step involves totalling up everything from the previous time period, storing the computed answers in another table, which has like one row per timeperiod, and then removing the rows about that time period from the granular table

[2e] this is an opportunity to try out another provider alongside supabase, like Neon, to see what the developer experience is like, and maybe it's faster
but, you code this for generic sql just like the regular database
these tables are also different because they drop rows

ok, based on all that, let's snapshot the design concept by looking at javascript function prototypes, and providers

log() - fancied up and sent to console.log
flare() - log to logflare, to see an error once pushed, and then remove it ~ uses logflare
dog() - goes to datadog, to add a note to the audit trail for real data in production ~ uses datadog

robinStart('email', 'amazon', tag) - we've asked aws to send a sms, and local code tracks the request with tag in a local variable
robinBad('email', 'amazon', tag) - some time later, the api denied our request
robinGood('email', 'amazon', tag) - the api and the user succeeded. oh here the tag local variable doesn't work anymore
robinChoose('sms') - returns which one to use, like 'amazon' or 'sendgrid' based on flipping a coin and avoiding errors

oh, there's a page at /staff that shows the performance of all of these
and make it so that to add a new one, you just start talking about it, you don't have to do anything else
because there are like two dozen tables named 001-012, and then an organizer table which holds the names
this is a really cool idea, you can get what you need in just a few screenfuls of code
~ uses supabase or neon/planetscale, one of the others

real pages pingdom and users can go to
/ping1 static page from cloudflare pages
/ping2 page that depends on a hello world cloudflare worker
/ping3 essentially your count button, page->worker->supabase settings table
~ try pingdom, datadog synthetics, and checkly, and see what you like

you're really happy with all of this
notice how much you got done just in .txt, not .js
this also completely avoids kinesis and kafka, which you might have dove into
also, instead of making log() more complex, log(), flare() and dog() are separate and simple this way

actually the robin interface is like this:

robin1 - about to call the api
robin2 - api returned reporting success
robin3 - user completed task proving success
robin4 - api returned refusal or failure

14 - the api hates us now, duration not really important here, "red fourteens"
123 - everything's working for the user, 13 duration is important "green onetwentythrees"
10 - the api never got back to us, really broken "red one nothings"
120 - the api says it worked, but the user never was able to do it or gave up "purple twelve nothings"

current minute - add records
previous minute - wait for users
two minutes ago - total up, but keep around for the daily digest. these three do the real time intelligence

current hour
previous hour
two hours ago - total up and move to archival table, which will grow one record per service per user, which is fine

a single lastcycle tick in the control table notices new minutes and hours, and performs the necessary totaling

more later:

you can totally do this efficiently and easily without sets of tables for provider
there are three tables
robin_control - settings just for robin
robin_hours - finished records of each service each hour, totaled and averaged for 1-2pm at the start of 3pm
robin_minutes - individual recent records

hours counts the 14s, 123s, 1nothing, and 12nothings
the 123s duration is meaningfull, hours includes the mean, median, standard deviation, and specific outliers

columns like:
service
provider
tick1
tick2
tick3
tick4
and then you select to total and average
and select to remove old rows only if there are too many rows

the intelligence to pick a service for a task isn't baked into the robin api, rather it's all in the worker, using that api
functions like robinRecent(service, provider) returns a js object with mean, median, failure rates, and so on
and from those results the business logic in the worker

maybe do to the minute, and then every 6 hour period, quarter day
so the site can be up years and the records don't get very huge
and an api that breaks can be detected really quickly and steered around

all you need to do are: 10, 50, 90th percentile value, those three, 50 is median of course
you don't need average, and you don't need slowest and fastest list

more several days later:

ok, but you also want to know how fast the site seems for users
here's a separate, additional, and still pretty simple way to do that

every api request is lumped together, those with no, small, and complex database interactions
telemetry is entirely from the worker, so the durations are trusted

the public system status page shows the 10/50/90 percentiles of response times in milliseconds
this goes beyond just saying, "everything's fine" and a bunch of granular, but detail-free, green checkmarks

chatgpt says i could make this with aws kinesis, but aws cloudwatch is a better fit
the example code uses cloudwatch, dynamodb, and lambda to store and then summarize

more later:

cloudwatch and datadog are actually designed for this
you like datadog in that it's separate and may be simpler
datadog can compute percentiles on the server, you don't have to download all the datapoint and run them yourself

// Define the query to get 10th, 50th, and 90th percentiles, count, sum, and last value
const query = `percentile:my_service.my_duration{my_tag:my_value}.rollup(60, p10, p50, p90), 
	count:my_service.my_duration{my_tag:my_value}.rollup(60),
	sum:my_service.my_duration{my_tag:my_value}.rollup(60),
	last:my_service.my_duration{my_tag:my_value}`
const params = new URLSearchParams({
	from: Math.floor(new Date(from).getTime() / 1000),
	to: Math.floor(new Date(to).getTime() / 1000),
	query: query})

not sure on use of rollup(), as_count(), or nothing on the ends
https://docs.datadoghq.com/dashboards/functions/rollup/#rollup-interval-enforced-vs-custom

you'll want to scrutinize the slow ones
you could do this by counting them, but also if it's slow, your robin() code logs them to a different tag or whatever of datadog, and the dashboard picks them up
this is easier and better than trying to search for them later






== big divide, done

some bike shed
fraction([1, 2, 3], [4])
always takes numerator and denomonator arrays, , [1] if you just want to multiply
always takes js numbers, not strings, not bigints
checks that they're all numbers and integers
multiplies top and bottom and then does the division, that part uses bigint so it can go over
throws if the answer is too big to fit as a regular js integer
returns an object of answers which include answer and remainder





== nuxt public

[]in the nuxt project, get rid of public/favicon.ico






== all the ways code can run

>all the ways code can run:

icarus libarary tests (vite, local, client)
node library tests (node, local, server)

nuxt server side api code (nuxt, local|deployed, server)
nuxt client side code running on the server as part of the hydration step (nuxt, local|deployed, server)
nuxt client side code running on the client (nuxt, local|deployed, client)

serverless framework lambda code running on the server (serverless, local|deployed, server)

(all those x these additional possibilities)

nuxt local development
nuxt deployed to cloudflare

serverless framework local lambda emulation
serverless framework deployed to aws lambda

>design

we're in the bike shed with this one, but wouldn't it be cool to have

a function you query that tells you where you are

the floppy disk ascii where you set
name and version, which are available to code everywhere

and then every time you git commit or upload push, a prerun task computes and writes
####s showing how much code you've written, on the disk
sha256 hash of all the code you've written, filled in each time
from the date and hash a simple version slug like v2024aug14h00FF00FF

 ____________________
| |cold3           | |
|.|________________|H|
| |________________| |
| |________________| |
| |________________| |
| |________________| |
| |________________| |
|                    |
|    ____________    |
|   |   |  _     |   |
|   |   | | |    |   |
|   |   | |_|    | V |
|___|___|________|___|

stamp task looks at all the files except .gitignore and diskette* to generate stamp1.js:

export stamp1 = {
	name: 'cold3',//set by user
	tick: 456789456789456,//from developtment system
	files: 456,//from disk
	bytes: 456789456123,//totaled from disk
	hash1: 'DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ'//computed from disk
}

and then looks at stamp1.js to generate disk2.js:

export stamp2 = {
	(add in the stuff from stamp1, also),
	hash2: 'QUPQBI7B4DSJCBGLG2G5FLB6NFFUM4DZNWS3YKE7UGMCNHQYUOGC',//computed from stamp1.js
	seal: '2024aug14_QUPQBI7B'//composed from stamp1.tick and stamp2.hash, like a wax seal over the whole thing
}

and then code everywhere in all three projects references and shows stamp2.stamp

this is called stamp, as in
$ npm run stamp
it doesn't happen automatically, you manually do stamp, deploy, push
so now you can match a git version on github with what the server shows in cold3.cc/about

this is a pretty cool idea

there are two files so you can put things back, but at a later time, and confirm you get the same stamp1.hash1

simple way to do this
$ npm run stamp
does node stamp
stamp.js loads the library itself
this is just a regular old fashioned node script
meant only to run on the development workstation

actually generate a stamp1.txt with hashes, sizes, and paths like:
DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ 123456 folder/file.js
DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ 123456 folder/file2.js

so the files in order are:
stamp1.txt, made from disk
stamp2.js, made from stamp1
stamp3.js, made from stamp2








