

























/*
very brief notes about logging:

do use console log and console error, they go to local terminal, amazon cloudwatch, and maybe later cloudflare, too

sinks include:
-icarus textarea
-browser inspector
-bash command line
-amazon dashboard
-node write file
-datadog

types of logs include:
-temporary for development, DEBUG, log()
-unusual to investigate, ALERT, logAlert()
-record of transaction, AUDIT, logAudit()

parts of a complete log:
-type, like DEBUG, ALERT, AUDIT
-tag, so you know if it's a different log or the same log twice
-tick, so you know when it happened, machine and human readable here also, please
-cloud true or not
-environment detection and tags
-note, one to three words
-longer message, composed message that describes easily
-human readable watch, like from look()
-machine complete watch, like from JSON.stringify()
-size of all that before you send it to datadog, so you know if this is going to impact your bill

log exceptions at the top, not at the bottom
so, not in toss(), but rather around door

[]get rid of stray logs cluttering things from months ago
[]get rid of the log record, only icarus is using it and doesn't need it
[]condense and move that essay you wrote in notes about logging, the one that includes ROBIN

general checks
-everywhere you call console log and console error directly, shouldn't they go through this system?

general questions
-what do you do with a caught exception after logging to datadog has failed?

future expeditions
-errors on the page, how do they get to datadog? through a fetch to api, i guess, but then they're not trusted? what's the right nuxt way to deal with these?
*/

/*
candidate design; map of functions and sinks:


  meaning, there was an uncaught exception indicating a mistake in old code
  meaning, would be an alert but likely we won't be able to record it
	these are async

  meaning, we did something consequential with a third party service, like send a text, or get denied sending a text
logAudit local: console log, file, dog, AUDIT
logAudit cloud: console log,       dog, AUDIT
  these are the same local and cloud, because the third party service didn't know the difference, we're sending real emails and texts here, and if we fuck it up, the real service provider will deplatform us
  these are async
*/



/*
took a step away, here are simplifications:
-there is no dog-specific say tick format! are you fing kidding me?! or, you'd want Real Local Time, like 9:10p5.789s but for right now, sayTick() is all you need, in Zulu, and you can find the minutes
-maybe someday in the future, you'll properly refactor all this, so that pieces are named below, and rise up, and are never split or parsed. but that day is not today
-a DOG log has that type, and then just the rest as text. it doesn't need to be well parsed, duh
*/





















/*
refactor to call this first
and then add properties from there

*/






//throw the sticker as an object into datadog, too; get the whole hash in there


//at the end of the first line say the byte size of the whole thing, like ‹2048›



/*
ok, the design here is there's a headline
and then beneath that, more lines generated by look()
and then you give to datadog an object that was stringified by your wrapped stringify()
oh wait that happens at the ashFetchum level, interesting

also be able to see this run in icarus, before you do the slow thing with deploying and checking datadog
*/




/*
copying here, an essay you wrote about loggin'

i want to use datadog for a variety of purposes. for instance, here are four:
(1 "robin") high frequency performance analysis: logs of different named attempts, their duration, and success, failure, or timeout. there could be a lot of these (many per second). also, the app will need to query them, to find out what's working quickly and reliably, and get percentiles over recent time periods
(2 "audit") verbose documentation of third party api performance: here, the logs will be longer, and contain json of objects that go perhaps several references deep. with this use case, there's no querying--this is for archival, only. later on, if an api is misbehaving, developers may go into datadog to look at this record to try to determine the cause
(3 "alert") important and immediate information for developers: let's say a truly exceptional exception occurs, like code that we wrote that's part of our app throws, in a way that should be impossible. this third category of logs (top level uncaught exceptions) needs to be extremely verbose, separate from the other two types, and immediately for the attention of the development team
(4 "debug") current development in deployed environment: when coding, a developer might use console.log to see some variables they're watching in code as it runs. then, when deployed, it can be useful to also see those kinds of logs. on the next push, these log statements might be removed. and, these logs are meant to be throwaway--they won't be saved, and they won't be consistent

*/



/*
should message end with look(watch)? to make it easier to read things in datadog, and because message will often just be one to three words

cosmetic chat
imagine message is only for you, the human, looking at datadog
anything that code will parse or sort is going to be elsewhere, not message

logFragile(title, watch) <- so pass in the title like 'short title'
and you keep that as title in the object you're logging, also
but then compose a verbose just for humans message like this:

Sat12:46p45.651s CloudLambda [ALERT] "short title" E6jj5g69BNWeqYebqRSxZ
{ <22>
	e: blah blah from look
	watch2: 17
}

here's where you say the type in all caps, like DEBUG
there's no arrow because its always on two lines

so taht covers
tick, environment, type, title, tag
watch

and maybe this consistant format is what goes to the other sinks, too
you already sorta did this for the node test log file

maybe instead of arrows, which would always be down, put the type in all caps and braces, like [DEBUG]





*/






/*
notes about how to correctly to fetch() to datadog

body is a stringified array of objects
you can have several objects; here we're just sending one

each object has required:
.ddsource
.service
.message, becomes text at the top, can be multiline
and reccommended:
.hostname
.status
.tags
(hi chat, other requirements or strong reccomendations?)

and then also has whatever additional application-specific properties you want to pin

example:

	bodyForDatadog = [{
		ddsource: 'my-service',
		service: 'user-service',
		hostname: 'cloudflare-worker-instance-1',
		message: logObject.message || 'Default log message',
		status: logObject.status || 'info',
		tags: logObject.tags || ['env:prod', 'region:us-west'],

		(and so on)
	}]

	await fetch(url, {
		method: 'POST',
		headers,
		body: JSON.stringify(bodyForDatadog)
*/








//to get here, there was an exception logging an exception--probably an import is missing, or maybe somehow a circular reference got to json stringify. it's possible that the code that follows will throw, too, so shout for help first, before trying to log full details next

//^make sure you include this excellent comment about logDiscard and FRAGILE









/*
	d.watch = watch
	d.message = (
		`${d.sticker.nowText} [FRAGILE] "${headline}" ${d.sticker.where}.${d.sticker.what} ${d.tag} ‹REPLACE_SIZE›`+newline
		+look(watch))
*/
	//at this point, notice how d.message contains all the information as text,
	//and d is all the information in both text and parsable objects
//there's some careful object manipulation here: pass an object we'll call c which has property o set to an array of one object. below we'll stringify that array and set the resulting text as the body in our fetch call to datadog













//	await dog('getting ready to send emails and texts')

/*
first, test tossing those 6 places
local, then cloud



try out now
node test: []email sendgrid, []sms twilio, []email amazon, []sms amazon
worker local: []email sendgrid, []sms twilio
worker deployed: []email sendgrid, []sms twilio
lambda local: []email sendgrid, []sms twilio, []email amazon, []sms amazon
lambda deployed: []email sendgrid, []sms twilio, []email amazon, []sms amazon
*/














































/*
bridges from the library, maybe:

cloud.js - all calls to fetch from our server code to third party apis
database.js - everything supabase, these two should work both places
amazon.js - everything that uses the node amazon modules

and so icarus probably can't cover tests that call amazon
but that's what $ node test.js is for?
*/









/*
summary note about Nuxt/Serverless, and useFetch(), $fetch(), and fetch()
here, because we're deep in the library, you're using fetch instead of Nuxt's $fetch
and thus need to add content type header and stringify the body

a nuxt component calling down to a nuxt api handler should use $fetch
a nuxt api handler can use $fetch
code that fetches that might be called by nuxt or serverless must use fetch

so then there's useFetch and $fetch
only use useFetch when you want hydration
which for 1.0 might be never!
after that may be just for info graph cards and then search engine optimization
useFetch is complicated because it returns already reactive variables
*/












//here also is where you deal with, what if the api takes forever
//you need to return a toolong
//and then much later if it does come back, log that you gave up, but the api did do something
//it's cool that you've figured out this design for fetch.js to be the place wehre you handle all of this
//also, there should be like a datadog endpoing which is just a critical and emergency report to developers
//and that's where all this stuff goes--not api failure, but our own code failure
//and very slow apis that meant the code here imagined the other side never got back to us, but then did!

//also, let's say it just never never gets back--does cloudflare stop the worker at some point?


/*
A note on fetch, exceptions, catch and throw, JSON parsing and stringification, and controlling what we can.

Exceptions from errors in our own code propagate upwards to be caught and logged at a higher level.
For example, JSON.stringify throws if it encounters a circular reference or a BigInt property.
But this is (1) highly unlikely and (2) indicates a serious error in our code.
So these exceptions are thrown up to be logged for us to fix.

Conversely, these functions are designed so that no matter how external APIs behave, they *cannot* throw up.
All issues related to API behavior and response are caught and returned as part of the result object, allowing the caller to handle them appropriately.
Unlike the coding errors mentioned earlier, these API-related issues are both (1) quite likely and (2) completely beyond our control.
The calling code will detect these issues, log them, and can implement round-robin failover to avoid relying on an API which was working great for weeks, and suddenly becomes problematic.

God, grant me the serenity to accept the things I cannot change,
Courage to change the things I can,
And wisdom to know the difference.
*/



test(() => {
//	log('hi2')


	//you just realized that icarus itself could probably send an email
	//not sure if you should do it that way, though
})




//sendEmail();


/*
what if you don't want to wait for the body
what if you dont' want to wait at all--ignore any response, and return as quickly as possible
what if the server takes forever, you give up, and then it reports having done something
what if the response isn't in json--there's probably a response header and you should only parse after checking it!


*/











/*
this low level server function just
-uses the api
-calls fetch
-never throws an exception, no matter what the api says or does
-returns an object of results for the caller to look through
*/


	/*
	url
	boolean dontWaitForResponse, dontWaitForBody
	*/





function scratch5() {

	//maybe expose your api like this, one big options object
	ashFetchum({
		resource: 'https://example.com/api/endpoint',
		dontWaitForResponse: true,
		dontWaitForBody: true,
		method: 'POST',
		headers: {
			Authorization: `Bearer ${key}`,
			'Content-Type': 'application/json'
		},
		bodyToStringify: {
			personalizations: [
				{
					to: [{ email: 'test@example.com' }] // Change to your recipient
				}
			],
			from: { email: 'test@example.com' }, // Change to your verified sender
			subject: 'Sending with SendGrid is Fun',
			content: [
				{
					type: 'text/plain',
					value: 'and easy to do anywhere, even with fetch'
				}
			]
		}
	})

}












/*
this function performs these steps:
-hash or encrypt user secrets, like credit card numbers and email addresses
-redact application secrets, like api keys, so they don't get compromised in logs
-remove redundant clutter from common objects
-measure the byte size of the payload we're logging to datadog, which leads to monetary cost

methods used include:
-knowing paths to secrets and properties to remove
-searching for key names across the whole tree
-searching through resulting text to blank out potentiall problem areas

changes and return
-edits d in place
-returns
*/


more thinking about redact
it's a difficult problem
you can't actually prune the tree, because those are the real objects! you need to use them, still!
if you blank out an api key, well, you did it in the real copy

ok, so you could stringify, then parse to make a copy, redact that, then stringify again
this probably works? but then there's the look version, ugh

so the look version can't be parsed
even if you could parse it, the type information would be lost--json stringify, parse turns everything into a regular object, there's no instanceof anymore

also, some of what you're trying to redace you know exactly what the value is
other stuff that you're trying to redact you know what the property name is

if you affect the whole

so this is what you came up with, but absolutely table this for far later, probably after v1
deal with the body text
you're going to redact it
but have to be careful, because it still needs to be json parsed by datadog

sort characters as follows:
letters, numbers, +.@, those are word charcters
everything else, those are format characters
you're not splitting, exactly, rather you're dividing into alternating strings of word/format/word/format
and then you scan through teh whole thing


well, this  may actually not be too hard, and may actually work
ask chat how json stringify works
oh no it's not going to work because of escaped characters, ugh
ok, yeah, it's hard

two different strategies
(1) for the look string
split into lines
for each line, parse out the key and the value
redact values based on value
redact values based on key
form back into lines, keeping the indent and last part

(2) for the stringified body
unstringify the body, copying it, eliminating types, undoing escaping
go through all the key-value pairs, looking at keys and redacting values
and looking just at values and redacting them
restringify the redacted copy

third design
you bake this feature into look and stringify
you already have both custom wrapped
and that way, you never have to parse anything
keys are always text, and you only need to scrutinize 
stringify already has key and value
you'll have to add that to look

this also gets ahead of the "too long <999> thing in look
weren't you going to put an ellipsis on there, actually?
[]like do ellipsis character, actually



here's a cheap/easy solution for right now
there are only a few 


for right now it's pretty easy





/*
this function performs these steps:
-hash or encrypt user secrets, like credit card numbers and email addresses
-redact application secrets, like api keys, so they don't get compromised in logs
-remove redundant clutter from common objects
-measure the byte size of the payload we're logging to datadog, which leads to monetary cost

methods used include:
-knowing paths to secrets and properties to remove
-searching for key names across the whole tree
-searching through resulting text to blank out potentiall problem areas

changes and return
-edits d in place
-returns
*/







/*
oh, here also is where you want to replace REPLACE_BODY_SIZE with the size
so much all caps these days!
*/


/*
you've checked your current set of secret values, and they contain letters, numbers, period, underscore, and hyphen
none of these get escaped by json stringify, so it works to search and replace secret strings of text in the stringified version





*/



